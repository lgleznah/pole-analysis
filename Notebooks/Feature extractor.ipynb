{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33e0b8e-e39a-48bd-af9f-c7033b39cd01",
   "metadata": {},
   "source": [
    "# Feature extraction for second groundtruth proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db478091-4a16-4c23-816a-cb79eb4b8d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import spectral.io.envi as envi\n",
    "import os\n",
    "import open3d as o3d\n",
    "\n",
    "from collections import defaultdict\n",
    "from skimage.filters import gabor_kernel\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.stats import gaussian_kde\n",
    "from spectral import principal_components\n",
    "from math import sqrt\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "CHUNK_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330f12b-67ad-407a-bab8-d09db5d83c67",
   "metadata": {},
   "source": [
    "With the second groundtruth proposal, the feature extraction process has to be slightly modified. Now, since each chunk of the tree is considered as an independent observational unit (due to the pole damage being more or less important depending on its vertical location), this process must also be applied to the pictures and to the LiDAR and hyperspectral scans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73250396-5f71-4fe5-adec-5177e6f57b4a",
   "metadata": {},
   "source": [
    "## RGB - chunkified features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6893d-0a5b-432f-acc4-1138707eb96d",
   "metadata": {},
   "source": [
    "For the RGB images, the first step is dividing the regions with pole into 10 different chunks along the vertical axis, and then extract the histograms as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809965ef-6f6c-4f12-98b5-c7c3a4f3ce09",
   "metadata": {},
   "source": [
    "### Gray histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0c7c2-9b01-4eff-b9b4-e87d72cfda81",
   "metadata": {},
   "source": [
    "Firstly, the gray histograms can be extracted from the poles without much trouble. However, the vertical borders of the crops have been further cropped by 400 pixels each, to remove artifacts that were present in the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f4073-46b1-4bcd-b12c-2c1e55880e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def resize_array(arr, final_size):\n",
    "    return cv2.resize(np.array([arr], dtype=np.float32), dsize=(final_size,1))[0]\n",
    "\n",
    "def crop(image, vertical_offsets=[0,0]):\n",
    "    y_nonzero, x_nonzero = np.nonzero(image)\n",
    "    return image[np.min(y_nonzero) + vertical_offsets[0]:np.max(y_nonzero) + vertical_offsets[1], np.min(x_nonzero):np.max(x_nonzero)]\n",
    "\n",
    "def crop_color(image, vertical_offsets=[0,0]):\n",
    "    y_nonzero, x_nonzero, _ = np.nonzero(image)\n",
    "    return image[np.min(y_nonzero) + vertical_offsets[0]:np.max(y_nonzero) + vertical_offsets[1], np.min(x_nonzero):np.max(x_nonzero),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877dc6ac-3fde-43d0-bb88-7ff2d9f2e9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_histograms_gray():\n",
    "    fig, axes = plt.subplots(5, 8, figsize=(20,16))\n",
    "    pole_ids = [0,30,41,6,5]\n",
    "    pole_status = ['31/37', '17/24', '15/24', '12/24', '11/24']\n",
    "    specific_status = ['New', 'Rotten', 'Cracks', 'Cracks', 'Rotten']\n",
    "    \n",
    "    result_dict = {\n",
    "        'pole_id': [],\n",
    "        'height_id': [],\n",
    "        'gray_hist_mean': [],\n",
    "        'gray_hist_std': []\n",
    "    }\n",
    "\n",
    "    for pole_idx, pole_id in enumerate(pole_ids):\n",
    "        intensity_values = []\n",
    "        for rotation_idx, rotation in enumerate([0, 90, 180, 270]):\n",
    "            img = cv2.imread(f'../../RGB/sam_crops/{pole_id}_{rotation}_masked.jpg')\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            img = crop(img, [400,-400])\n",
    "            intensity_along_vertical = np.sum(img, axis=1) / np.sum(img != 0, axis=1)\n",
    "            \n",
    "            # Plot. just for visualization purposes\n",
    "            axes[pole_idx, rotation_idx*2].plot(range(len(intensity_along_vertical)), intensity_along_vertical, color='black')\n",
    "            axes[pole_idx, rotation_idx*2].set_ylim([0,128])\n",
    "            axes[pole_idx, rotation_idx*2+1].imshow(img, cmap='gray')\n",
    "            axes[pole_idx, rotation_idx*2+1].set_xticks([])\n",
    "            axes[pole_idx, rotation_idx*2+1].set_yticks([])\n",
    "            \n",
    "            # Resize intensity curve and add to intensity values matrix\n",
    "            intensity_resized = resize_array(intensity_along_vertical, 2000)\n",
    "            intensity_values.append(intensity_resized)\n",
    "        \n",
    "        # Compute mean and stdev on each chunk\n",
    "        intensity_values = np.array(intensity_values)\n",
    "        chunks = np.array_split(intensity_values, CHUNK_SIZE, axis=1)\n",
    "        means, stdevs = [np.mean(chunk) for chunk in chunks] , [np.std(chunk) for chunk in chunks]\n",
    "        pole_id_dict = 0 if pole_id=='Ny' else int(pole_id)\n",
    "        result_dict['pole_id'] += ([pole_id_dict] * CHUNK_SIZE)\n",
    "        result_dict['height_id'] += list(range(CHUNK_SIZE))\n",
    "        result_dict['gray_hist_mean'] += means\n",
    "        result_dict['gray_hist_std'] += stdevs\n",
    "        \n",
    "    fig.suptitle('Color histograms')\n",
    "\n",
    "    [axes[0,i*2].set_title(f'{i*90} degrees') for i in range(4)]\n",
    "\n",
    "    [axes[i,0].set_ylabel(f'Pole {pole}\\nStatus: {pole_status[i]}\\nCause: {specific_status[i]}', labelpad=60, fontdict={'rotation':0}) for i, pole in enumerate(pole_ids)]\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91e3c3-14c0-4799-8088-170e22afc6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gray_hist_data = visualize_histograms_gray()\n",
    "df_gray = pd.DataFrame(gray_hist_data)\n",
    "df_gray.to_csv(f'../Features/second_proposed_gt/gray_histograms.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51734bd1-375e-4256-a7b1-f07d3b48edef",
   "metadata": {},
   "source": [
    "### Color histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf91bd-5511-4a2c-bc88-437482f4d498",
   "metadata": {},
   "source": [
    "For the color histograms, the procedure is basically the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db812ea9-e72a-4201-b643-67c8da7877a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_histograms_color():\n",
    "    fig, axes = plt.subplots(5, 8, figsize=(20,16))\n",
    "    pole_ids = [0,30,41,6,5]\n",
    "    pole_status = ['31/37', '17/24', '15/24', '12/24', '11/24']\n",
    "    specific_status = ['New', 'Rotten', 'Cracks', 'Cracks', 'Rotten']\n",
    "    \n",
    "    result_dict = {\n",
    "        'pole_id': [],\n",
    "        'height_id': [],\n",
    "        'red_hist_mean': [],\n",
    "        'red_hist_std': [],\n",
    "        'green_hist_mean': [],\n",
    "        'green_hist_std': [],\n",
    "        'blue_hist_mean': [],\n",
    "        'blue_hist_std': [],\n",
    "    }\n",
    "\n",
    "    for pole_idx, pole_id in enumerate(pole_ids):\n",
    "        intensity_values_r = []\n",
    "        intensity_values_g = []\n",
    "        intensity_values_b = []\n",
    "        for rotation_idx, rotation in enumerate([0, 90, 180, 270]):\n",
    "            img = cv2.imread(f'../../RGB/sam_crops/{pole_id}_{rotation}_masked.jpg')\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = crop_color(img, [400,-400])\n",
    "            intensity_along_vertical = np.sum(img, axis=1) / np.sum(img != 0, axis=1)\n",
    "            \n",
    "            # Plot. just for visualization purposes\n",
    "            axes[pole_idx, rotation_idx*2].plot(range(len(intensity_along_vertical[:,0])), intensity_along_vertical[:,0], color='red')\n",
    "            axes[pole_idx, rotation_idx*2].plot(range(len(intensity_along_vertical[:,1])), intensity_along_vertical[:,1], color='green')\n",
    "            axes[pole_idx, rotation_idx*2].plot(range(len(intensity_along_vertical[:,2])), intensity_along_vertical[:,2], color='blue')\n",
    "            axes[pole_idx, rotation_idx*2].set_ylim([0,128])\n",
    "            axes[pole_idx, rotation_idx*2+1].imshow(img, cmap='gray')\n",
    "            axes[pole_idx, rotation_idx*2+1].set_xticks([])\n",
    "            axes[pole_idx, rotation_idx*2+1].set_yticks([])\n",
    "            \n",
    "            # Resize intensity curve and add to intensity values matrix\n",
    "            intensity_values_r.append(resize_array(intensity_along_vertical[:,0], 2000))\n",
    "            intensity_values_g.append(resize_array(intensity_along_vertical[:,1], 2000))\n",
    "            intensity_values_b.append(resize_array(intensity_along_vertical[:,2], 2000))\n",
    "        \n",
    "        # Compute mean and stdev on each chunk\n",
    "        intensity_values_r, intensity_values_g, intensity_values_b = np.array(intensity_values_r), np.array(intensity_values_g), np.array(intensity_values_b)\n",
    "        chunks_r = np.array_split(intensity_values_r, CHUNK_SIZE, axis=1)\n",
    "        chunks_g = np.array_split(intensity_values_g, CHUNK_SIZE, axis=1)\n",
    "        chunks_b = np.array_split(intensity_values_b, CHUNK_SIZE, axis=1)\n",
    "        means_r, stdevs_r = [np.mean(chunk) for chunk in chunks_r] , [np.std(chunk) for chunk in chunks_r]\n",
    "        means_g, stdevs_g = [np.mean(chunk) for chunk in chunks_g] , [np.std(chunk) for chunk in chunks_g]\n",
    "        means_b, stdevs_b = [np.mean(chunk) for chunk in chunks_b] , [np.std(chunk) for chunk in chunks_b]\n",
    "        pole_id_dict = 0 if pole_id=='Ny' else int(pole_id)\n",
    "        result_dict['pole_id'] += ([pole_id_dict] * CHUNK_SIZE)\n",
    "        result_dict['height_id'] += list(range(CHUNK_SIZE))\n",
    "        result_dict['red_hist_mean'] += means_r\n",
    "        result_dict['red_hist_std'] += stdevs_r\n",
    "        result_dict['green_hist_mean'] += means_g\n",
    "        result_dict['green_hist_std'] += stdevs_g\n",
    "        result_dict['blue_hist_mean'] += means_b\n",
    "        result_dict['blue_hist_std'] += stdevs_b\n",
    "        \n",
    "    fig.suptitle('Color histograms')\n",
    "\n",
    "    [axes[0,i*2].set_title(f'{i*90} degrees') for i in range(4)]\n",
    "\n",
    "    [axes[i,0].set_ylabel(f'Pole {pole}\\nStatus: {pole_status[i]}\\nCause: {specific_status[i]}', labelpad=60, fontdict={'rotation':0}) for i, pole in enumerate(pole_ids)]\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d2334-6944-4923-a0d1-94a06bec27f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_hist_data = visualize_histograms_color()\n",
    "df_color = pd.DataFrame(color_hist_data)\n",
    "df_color.to_csv(f'../Features/second_proposed_gt/color_histograms.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b73d71b-5414-45c9-bb8a-8800eaa06665",
   "metadata": {},
   "source": [
    "### Gabor filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c3b69-5820-47c2-9571-34f27faa4af6",
   "metadata": {},
   "source": [
    "The procedure for obtaining the chunkified Gabor filters features will be similar to what has been done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a439e-245d-4105-9d2d-86a41601477c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Skimage version 0.19.3\n",
    "# OpenCV version 4.6.0\n",
    "\n",
    "kernel_params = [\n",
    "    ('skimage', (0, 12, 0.01, 0)),\n",
    "    ('skimage', (0, 12, 0.05, 0)),\n",
    "    ('skimage', (0, 12, 0.25, 0)),\n",
    "    ('skimage', (1, 12, 0.01, 0)),\n",
    "    ('skimage', (1, 12, 0.05, 0)),\n",
    "    ('skimage', (1, 12, 0.25, 0)),\n",
    "    ('skimage', (2, 12, 0.01, 0)),\n",
    "    ('skimage', (2, 12, 0.05, 0)),\n",
    "    ('skimage', (2, 12, 0.25, 0)),\n",
    "    ('skimage', (3, 12, 0.01, 0)),\n",
    "    ('skimage', (3, 12, 0.05, 0)),\n",
    "    ('skimage', (3, 12, 0.25, 0)),\n",
    "    ('cv2', ((500, 1000), 100, 0, 1, 0.4, np.pi, 2, False)),\n",
    "    ('cv2', ((500, 1000), 100, 0, 1, 0.4, np.pi, 4, False)),\n",
    "    ('cv2', ((500, 1000), 100, 0, 1, 0.4, np.pi, 8, False)),\n",
    "    ('cv2', ((500, 1000), 100, 0, 1, 0.4, np.pi, 2, True)),\n",
    "    ('cv2', ((500, 1000), 100, 0, 1, 0.4, np.pi, 4, True)),\n",
    "    ('cv2', ((500, 1000), 100, 0, 1, 0.4, np.pi, 8, True)),\n",
    "]\n",
    "\n",
    "def power(image, kernel):\n",
    "        # Normalize images for better comparison.\n",
    "        image = (image - image.mean()) / image.std()\n",
    "        return np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 + ndi.convolve(image, np.imag(kernel), mode='wrap')**2)\n",
    "\n",
    "def plot_activations_kernel_pole_rotation(backend, params, kernel_id, draw=True):\n",
    "    if draw:\n",
    "        fig, axes = plt.subplots(5, 8, figsize=(20,16))\n",
    "    \n",
    "    # Generate Gabor kernel depending on specified backend\n",
    "    if backend == 'skimage':\n",
    "        theta = params[0] / 4. * np.pi\n",
    "        kernel = gabor_kernel(params[2], theta=theta, sigma_x=params[1], sigma_y=params[1], offset=params[3])\n",
    "    elif backend == 'cv2':\n",
    "        kernel_params, inv_scale, transpose = params[:-2], params[-2], params[-1]\n",
    "        kernel = cv2.getGaborKernel(*kernel_params)\n",
    "        if transpose:\n",
    "            kernel = kernel.T\n",
    "            \n",
    "    # Compute kernel statistics for all pole figures\n",
    "    pole_ids = [0,30,41,6,5]\n",
    "    rotations = [0,90,180,270]\n",
    "    pole_status = ['31/37', '17/24', '15/24', '12/24', '11/24']\n",
    "    specific_status = ['New', 'Rotten', 'Cracks', 'Cracks', 'Rotten']\n",
    "    \n",
    "    # Prepare dictionaries to store features\n",
    "    result_dict = {\n",
    "        'pole_id': [],\n",
    "        'height_id': [],\n",
    "        f'kernel_{kernel_id}_mean': [],\n",
    "        f'kernel_{kernel_id}_std': [],\n",
    "        f'kernel_{kernel_id}_max': [],\n",
    "        f'kernel_{kernel_id}_min': [],\n",
    "    }\n",
    "    \n",
    "    for pole_idx, pole in enumerate(pole_ids):\n",
    "        activation_curves = []\n",
    "        for rotation_idx, rotation in enumerate(rotations):\n",
    "            # Read, crop, and rescale image\n",
    "            img = cv2.imread(f'../../RGB/sam_crops/{pole}_{rotation}_masked.jpg')\n",
    "            mask = cv2.imread(f'../../RGB/sam_crops/{pole}_{rotation}_mask.jpg')[:,:,0]\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            img = cv2.blur(img, (10,10))\n",
    "            img = crop(img)\n",
    "            mask = crop(mask)\n",
    "            scale_percent = 15\n",
    "            width = int(img.shape[1] * scale_percent / 100)\n",
    "            height = int(img.shape[0] * scale_percent / 100)\n",
    "            dim = (width, height)\n",
    "            img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "            mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "            # Properly rescale kernel if it is from OpenCV\n",
    "            if backend == 'cv2':\n",
    "                kernel = imutils.resize(kernel, width = width // inv_scale)       \n",
    "            \n",
    "            activations = np.real(power(img, kernel))\n",
    "            activations_masked = np.where(mask == 255, activations, 0)\n",
    "            remapped_activations = np.interp(activations_masked, (activations_masked.min(), activations_masked.max()), (0, +1))\n",
    "            activation_along_vertical = np.mean(remapped_activations, axis=1)\n",
    "            activation_along_vertical = resize_array(activation_along_vertical, 2000)\n",
    "            \n",
    "            axes[pole_idx, rotation_idx*2].plot(range(len(activation_along_vertical)), activation_along_vertical)\n",
    "            axes[pole_idx, rotation_idx*2].set_xticks([])\n",
    "            axes[pole_idx, rotation_idx*2].set_ylim([-0.1,1.1])\n",
    "            axes[pole_idx, rotation_idx*2+1].imshow(activations_masked)\n",
    "            axes[pole_idx, rotation_idx*2+1].set_xticks([])\n",
    "            axes[pole_idx, rotation_idx*2+1].set_yticks([])\n",
    "\n",
    "            # Print mean and standard deviation to compare plots\n",
    "            activation_curves.append(activation_along_vertical)\n",
    "        activation_curves = np.array(activation_curves)\n",
    "        chunks = np.array_split(activation_curves, CHUNK_SIZE, axis=1)\n",
    "        means, stdevs, c_max, c_min = [np.mean(chunk) for chunk in chunks] , [np.std(chunk) for chunk in chunks] , [np.max(chunk) for chunk in chunks] , [np.min(chunk) for chunk in chunks]\n",
    "        pole_id_dict = 0 if pole=='Ny' else int(pole)\n",
    "        result_dict['pole_id'] += ([pole_id_dict] * CHUNK_SIZE)\n",
    "        result_dict['height_id'] += list(range(CHUNK_SIZE))\n",
    "        result_dict[f'kernel_{kernel_id}_mean'] += means\n",
    "        result_dict[f'kernel_{kernel_id}_std'] += stdevs\n",
    "        result_dict[f'kernel_{kernel_id}_max'] += c_max\n",
    "        result_dict[f'kernel_{kernel_id}_min'] += c_min\n",
    "                \n",
    "    fig.suptitle('Gabor filter activation histograms')\n",
    "    [axes[0,i*2].set_title(f'{i*90} degrees') for i in range(4)]\n",
    "    [axes[i,0].set_ylabel(f'Pole {pole}\\nStatus: {pole_status[i]}\\nCause: {specific_status[i]}', labelpad=60, fontdict={'rotation':0}) for i, pole in enumerate(pole_ids)]\n",
    "    plt.savefig(f'../Figures/Gabor/boxplots/kernel_{kernel_id}_boxplots.png')\n",
    "    plt.close()\n",
    "    fig, axes = plt.subplots()\n",
    "    axes.imshow(np.real(kernel), cmap='gray')\n",
    "    axes.set_yticks([])\n",
    "    axes.set_xticks([])\n",
    "    plt.savefig(f'../Figures/Gabor/boxplots/kernel_{kernel_id}.png')\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2519ff1-29a7-41ef-ba58-6034e2238a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with plt.ioff():\n",
    "    for kernel_id, params in enumerate(kernel_params):\n",
    "        data = plot_activations_kernel_pole_rotation(*params, kernel_id)\n",
    "        df_kernel = pd.DataFrame(data)\n",
    "        df_kernel.to_csv(f'../Features/second_proposed_gt/gabor_kernel_{kernel_id}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53adb2-1228-446e-a011-183852e816d2",
   "metadata": {},
   "source": [
    "## Hyperspectral data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ca6c2-a68c-41fc-87c3-e3da4085de6d",
   "metadata": {},
   "source": [
    "Just as it has been done for the RGB data, a similar process can be applied to the hyperspectral data. After all, what is a hyperspectral scan but an RGB image with hundreds of channels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9014fd7-b4bf-41aa-a353-760a071fa14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_scan(img, mask):\n",
    "    img = np.where(img > 0.025, 0, img)\n",
    "    img = np.where(mask[:,:,0] > 0, np.transpose(img, [2,0,1]), 0)\n",
    "    img = np.transpose(img, [1,2,0])\n",
    "    \n",
    "    return img\n",
    "\n",
    "def get_pca_model():\n",
    "    pole_ids = [0,30,41,6,5]\n",
    "    pca = IncrementalPCA(n_components=10, batch_size=1)\n",
    "    for pole_idx, pole_id in enumerate(pole_ids):\n",
    "        paths = os.listdir(os.path.join(\"C:\\\\Users\\\\ext-lugo\\\\Hyperspectral\\\\Radiance\", str(pole_id)))\n",
    "        paths = [path for path in paths if \"float32.hdr\" in path]\n",
    "        paths.sort()\n",
    "        for scan_idx in range(len(paths)):\n",
    "            print(f\"Pole {pole_id}, scan number {scan_idx}\")\n",
    "            img = envi.open(os.path.join(\"C:\\\\Users\\\\ext-lugo\\\\Hyperspectral\\\\Radiance\", str(pole_id), paths[scan_idx])).load()\n",
    "            mask = cv2.imread(os.path.join(\"../../Hyperspectral_masks/\", f\"{str(pole_id)}_{str(scan_idx)}_mask.jpg\"))\n",
    "            white = img[1800:2200,1000:1100,:]\n",
    "            refl_img = img / np.mean(white, axis=(0,1))\n",
    "            wls = np.asarray(img.metadata['wavelength'], float)\n",
    "            img_feat = mask_scan(refl_img, mask)\n",
    "            \n",
    "            pca.partial_fit(np.reshape(img_feat, (img_feat.shape[0] * img_feat.shape[1], img_feat.shape[2])))\n",
    "            \n",
    "    return pca\n",
    "\n",
    "def view_hyperspectral_chemical(pca):\n",
    "    fig, axes = plt.subplots(5, 6, figsize=(40,12))\n",
    "    pole_ids = [0,30,41,6,5]\n",
    "    pole_status = ['31/37', '17/24', '15/24', '12/24', '11/24']\n",
    "    specific_status = ['New', 'Rotten', 'Cracks', 'Cracks', 'Rotten']\n",
    "    \n",
    "    result_dict = {\n",
    "        'pole_id': [],\n",
    "        'height_id': [],\n",
    "        f'hyper_pca_0_mean': [],\n",
    "        f'hyper_pca_0_std': [],\n",
    "        f'hyper_pca_1_mean': [],\n",
    "        f'hyper_pca_1_std': [],\n",
    "        f'hyper_pca_2_mean': [],\n",
    "        f'hyper_pca_2_std': [],\n",
    "    }\n",
    "    \n",
    "    for pole_idx, pole_id in enumerate(pole_ids):\n",
    "        paths = os.listdir(os.path.join(\"C:\\\\Users\\\\ext-lugo\\\\Hyperspectral\\\\Radiance\", str(pole_id)))\n",
    "        paths = [path for path in paths if \"float32.hdr\" in path]\n",
    "        paths.sort()\n",
    "        activation_accumulator_0 = []\n",
    "        activation_accumulator_1 = []\n",
    "        activation_accumulator_2 = []\n",
    "        for scan_idx in range(len(paths)):\n",
    "            print(f\"Pole {pole_id}, scan number {scan_idx}\")\n",
    "            img = envi.open(os.path.join(\"C:\\\\Users\\\\ext-lugo\\\\Hyperspectral\\\\Radiance\", str(pole_id), paths[scan_idx])).load()\n",
    "            mask = cv2.imread(os.path.join(\"../../Hyperspectral_masks/\", f\"{str(pole_id)}_{str(scan_idx)}_mask.jpg\"))\n",
    "            white = img[1800:2200,1000:1100,:]\n",
    "            refl_img = img / np.mean(white, axis=(0,1))\n",
    "            wls = np.asarray(img.metadata['wavelength'], float)\n",
    "            refl_img = mask_scan(refl_img, mask)\n",
    "            \n",
    "            img_feat = np.reshape(pca.transform(np.reshape(refl_img, (refl_img.shape[0] * refl_img.shape[1], refl_img.shape[2]))), (refl_img.shape[0], refl_img.shape[1], 10))\n",
    "            activation_along_vertical = np.mean(img_feat, axis=1)\n",
    "            activation_accumulator_0.append(resize_array(activation_along_vertical[:,0], 2000))\n",
    "            activation_accumulator_1.append(resize_array(activation_along_vertical[:,1], 2000))\n",
    "            activation_accumulator_2.append(resize_array(activation_along_vertical[:,2], 2000))\n",
    "            img_feat[:,:,0] = np.interp(img_feat[:,:,0], (np.min(img_feat[:,:,0]), np.max(img_feat[:,:,0])), (0,1))\n",
    "            img_feat[:,:,1] = np.interp(img_feat[:,:,1], (np.min(img_feat[:,:,1]), np.max(img_feat[:,:,1])), (0,1))\n",
    "            img_feat[:,:,2] = np.interp(img_feat[:,:,2], (np.min(img_feat[:,:,2]), np.max(img_feat[:,:,2])), (0,1))\n",
    "            axes[pole_idx, scan_idx].imshow(img_feat[...,3], vmin=np.min(img_feat), vmax=np.max(img_feat))\n",
    "            \n",
    "        activation_accumulator_0, activation_accumulator_1, activation_accumulator_2 = np.array(activation_accumulator_0), np.array(activation_accumulator_1), np.array(activation_accumulator_2)\n",
    "        chunks_0, chunks_1, chunks_2 = np.array_split(activation_accumulator_0, CHUNK_SIZE, axis=1), np.array_split(activation_accumulator_1, CHUNK_SIZE, axis=1), np.array_split(activation_accumulator_2, CHUNK_SIZE, axis=1)\n",
    "        means_0, stdevs_0 = [np.mean(chunk) for chunk in chunks_0] , [np.std(chunk) for chunk in chunks_0]\n",
    "        means_1, stdevs_1 = [np.mean(chunk) for chunk in chunks_1] , [np.std(chunk) for chunk in chunks_1]\n",
    "        means_2, stdevs_2 = [np.mean(chunk) for chunk in chunks_2] , [np.std(chunk) for chunk in chunks_2]\n",
    "        pole_id_dict = 0 if pole_id=='Ny' else int(pole_id)\n",
    "        result_dict['pole_id'] += ([pole_id_dict] * CHUNK_SIZE)\n",
    "        result_dict['height_id'] += list(range(CHUNK_SIZE))\n",
    "        result_dict[f'hyper_pca_0_mean'] += means_0\n",
    "        result_dict[f'hyper_pca_0_std'] += stdevs_0\n",
    "        result_dict[f'hyper_pca_1_mean'] += means_1\n",
    "        result_dict[f'hyper_pca_1_std'] += stdevs_1\n",
    "        result_dict[f'hyper_pca_2_mean'] += means_2\n",
    "        result_dict[f'hyper_pca_2_std'] += stdevs_2\n",
    "        \n",
    "    fig.suptitle('Hyperspectral chemical variation and PCA')\n",
    "\n",
    "    [axes[0,i].set_title(f'Image #{i}') for i in range(6)]\n",
    "\n",
    "    [axes[i,0].set_ylabel(f'Pole {pole}\\nStatus: {pole_status[i]}\\nCause: {specific_status[i]}', labelpad=60, fontdict={'rotation':0}) for i, pole in enumerate(pole_ids)]\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d324e-7095-432f-b302-82e384095a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = get_pca_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf06f5-9bea-48c7-a4e1-be0537576893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_hyper = view_hyperspectral_chemical(pca)\n",
    "df_hyper = pd.DataFrame(data_hyper)\n",
    "df_hyper.to_csv(f'../Features/second_proposed_gt/hyperspectral.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc94e9e-08c0-407b-85ed-57d2af3bb2d6",
   "metadata": {},
   "source": [
    "## LiDAR scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7deb4f9-d38e-44d6-8a47-f4842588e6d4",
   "metadata": {},
   "source": [
    "Now that the new LiDAR scans of all poles are available, features can also be extracted from these scans. More specifically, an accurate diameter estimate can be given for each chunk of the pole, and this estimate seems to be sensitive to certain pole surface characteristics, such as rot or defects due to cracks. \n",
    "\n",
    "Even if the LiDAR scan for pole 30 actually gets some points inside its big crack, and this in turn influences the diameter estimate, this is not neccesarily a bad thing for the estimation; even if this issue can influence the estimates, it can do so in a way that actually reflects the fact that the pole is in very bad condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae58651-2abf-4959-89a4-182cb39d7973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_continuous_mode(estimates):\n",
    "    distribution = gaussian_kde(estimates)\n",
    "    x_domain = np.linspace(min(estimates), max(estimates), 1000)\n",
    "    y_pdf = distribution.pdf(x_domain)\n",
    "    i = np.argmax(y_pdf)\n",
    "    return x_domain[i]\n",
    "\n",
    "\n",
    "def estimate_diameter(pcd, height_threshold):\n",
    "    estimates = []\n",
    "    for point in pcd.points:\n",
    "        points_within_threshold = [p for p in pcd.points if p[2] < point[2]+height_threshold and p[2] > point[2]-height_threshold]\n",
    "        points_distances = [math.dist(point, p) for p in points_within_threshold]\n",
    "        idx_max_distance = np.argmax(points_distances)\n",
    "        max_distance = points_distances[idx_max_distance]\n",
    "        furthest_point_z_diff = abs(points_within_threshold[idx_max_distance][2] - point[2])\n",
    "        diameter_estimate = sqrt(max_distance**2 - furthest_point_z_diff**2)\n",
    "        estimates.append(diameter_estimate)\n",
    "        \n",
    "    mu, sigma, mode = np.mean(estimates), np.std(estimates), get_continuous_mode(estimates)\n",
    "    \n",
    "    return estimates, mu, sigma, mode\n",
    "\n",
    "def get_lidar_features(height_threshold):\n",
    "    pole_ids = [0,30,41,6,5]\n",
    "    scan_names = ['poleny_2_ransac_crop', 'pole30_2_ransac_crop', 'pole41_1_all_ransac_crop', 'pole6_1_ransac_crop', 'pole5_2_ransac_crop']\n",
    "    result_dict = {\n",
    "        'pole_id': [],\n",
    "        'height_id': [],\n",
    "        f'diameter_mean': [],\n",
    "        f'diameter_std': [],\n",
    "        f'diameter_mode': []\n",
    "    }\n",
    "    poles_dir = '../../LiDAR/'\n",
    "    \n",
    "    for pole_id, scan_path in zip(pole_ids, scan_names):\n",
    "        pcd = o3d.io.read_point_cloud(os.path.join(poles_dir, f'NEW_LIDAR/{scan_path}.pcd'))\n",
    "        fig, axes = plt.subplots(ncols=CHUNK_SIZE, figsize=(5*CHUNK_SIZE, 5))\n",
    "        pcd_array = np.asarray(pcd.points)\n",
    "        min_height, max_height = np.min(pcd_array[:,2]), np.max(pcd_array[:,2])\n",
    "        chunk_size = (max_height - min_height) / CHUNK_SIZE\n",
    "        chunks = [[point for point in pcd_array if ((min_height + chunk_size*i) < point[2] < (min_height + chunk_size*(i+1)))] for i in range(CHUNK_SIZE)]\n",
    "        mus, sigmas, modes = [], [], []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            pcd_chunk = o3d.geometry.PointCloud()\n",
    "            pcd_chunk.points = o3d.utility.Vector3dVector(chunk)\n",
    "            estimates, mu, sigma, mode = estimate_diameter(pcd_chunk, height_threshold)\n",
    "            mus.append(mu)\n",
    "            sigmas.append(sigma)\n",
    "            modes.append(mode)\n",
    "            axes[idx].hist(estimates)\n",
    "            print(f'Diameter estimation for chunk {idx}: {mu:.3f} ± {sigma:.3f}, mode: {mode:.3f}')\n",
    "        result_dict['pole_id'] += ([pole_id] * CHUNK_SIZE)\n",
    "        result_dict['height_id'] += list(range(CHUNK_SIZE))\n",
    "        result_dict['diameter_mean'] += mus\n",
    "        result_dict['diameter_std'] += sigmas\n",
    "        result_dict['diameter_mode'] += modes\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb937f9-dc8c-4421-807a-32254ae4332b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_lidar = get_lidar_features(0.01)\n",
    "df_lidar = pd.DataFrame(data_lidar)\n",
    "df_lidar.to_csv(f'../Features/second_proposed_gt/lidar.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed9617-0908-4b52-a144-ced8db690899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
