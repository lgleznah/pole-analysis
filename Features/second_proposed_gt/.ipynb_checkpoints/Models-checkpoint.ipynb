{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc9d185-3dac-49d1-8ebf-f49ff834c6f7",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece3bae-4d46-4a8a-9ef6-c862d32aace8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import LeaveOneOut, LeavePOut, RandomizedSearchCV, GridSearchCV, cross_val_score, KFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_regression\n",
    "from collections.abc import MutableMapping\n",
    "\n",
    "from scipy.stats import lognorm, uniform, randint\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "RS = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ef425-d0f1-42a6-8f2d-74b18b8fdc6e",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49524233-9442-49bc-8a61-88ab8dee8ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read full DF if it is present, merge all other DFs instead\n",
    "def load_csv():\n",
    "    if ('full_df.csv' not in os.listdir()):\n",
    "        df_list = []\n",
    "\n",
    "        df_paths = [path for path in os.listdir() if path.lower().endswith('.csv') and path != \"full_df.csv\"]\n",
    "        for path in df_paths:\n",
    "            df = pd.read_csv(path, index_col=['pole_id', 'height_id'])\n",
    "            df_list.append(df)\n",
    "\n",
    "        full_df = pd.concat(df_list, axis=1, join='inner').reset_index()\n",
    "        full_df.to_csv('full_df.csv')\n",
    "\n",
    "    else:\n",
    "        full_df = pd.read_csv('full_df.csv').drop('Unnamed: 0', axis=1)\n",
    "        \n",
    "    return full_df\n",
    "\n",
    "def dict_product(dicts):\n",
    "    return list(dict(zip(dicts, x)) for x in itertools.product(*dicts.values()))\n",
    "\n",
    "def flatten(dictionary, parent_key='', separator='_'):\n",
    "    items = []\n",
    "    for key, value in dictionary.items():\n",
    "        new_key = key\n",
    "        if isinstance(value, MutableMapping):\n",
    "            items.extend(flatten(value, new_key, separator=separator).items())\n",
    "        else:\n",
    "            items.append((new_key, value))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807abbfa-c312-46a2-90fd-370ebe1e9ed8",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4878012-07e7-42bc-941b-b5cc41cc7749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_df = load_csv()\n",
    "X, y_surf, y_mid, y_int, y_diam, y_sdepth = full_df.iloc[:,2:-6], full_df.iloc[:,-6], full_df.iloc[:,-5], full_df.iloc[:,-4], full_df.iloc[:,-2], full_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ef0cc-8e63-4535-9e62-d5744b135f2c",
   "metadata": {},
   "source": [
    "# Experiments - pole surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739ec83-3032-44a6-b780-71d70b625eef",
   "metadata": {},
   "source": [
    "## Experiment #1: Scaling + linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbcd7bb-ff09-4920-abcd-2a46d34c6240",
   "metadata": {},
   "source": [
    "The most naïve approach is to just apply scaling to the dataset variables, and then training some linear model on top of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48719aa-f9fc-42f0-95ac-d69978477c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_exp(exp_name, target, pipe, param_grid, randomize=False, rs_iters=250, random_state=42, mae_plot_padding = True):\n",
    "    # Directory for plots\n",
    "    if not os.path.exists(f'plots/{exp_name}'):\n",
    "        os.mkdir(f'plots/{exp_name}')\n",
    "    \n",
    "    # Create and fit the model. Use outer CV to give honest estimates. Use inner CV to estimate hyperparams.\n",
    "    cv_outer = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    cv_outer_results_mae = []\n",
    "    cv_outer_results_r2 = []\n",
    "    \n",
    "    best_estimator = None\n",
    "    best_mae = float('inf')\n",
    "\n",
    "    def get_gs():\n",
    "        if not randomize:\n",
    "            gs = GridSearchCV(pipe, param_grid, cv=cv_inner, scoring=['explained_variance', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'r2'], refit='neg_mean_absolute_error', n_jobs=-1)\n",
    "        else:\n",
    "            gs = RandomizedSearchCV(pipe, param_grid, cv=cv_inner, scoring=['explained_variance', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'r2'], refit='neg_mean_absolute_error', random_state=random_state, n_iter=rs_iters, n_jobs=-1)\n",
    "\n",
    "        return gs\n",
    "    \n",
    "    for train_ix, test_ix in cv_outer.split(X):\n",
    "        # Split data\n",
    "        X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "        y_train, y_test = target.iloc[train_ix], target.iloc[test_ix]\n",
    "        \n",
    "        cv_inner = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "        gs = get_gs()\n",
    "\n",
    "        result = gs.fit(X_train, y_train)\n",
    "        \n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = result.best_estimator_\n",
    "        # evaluate model on the hold out dataset\n",
    "        yhat = best_model.predict(X_test)\n",
    "        # evaluate the model\n",
    "        mae = mean_absolute_error(y_test, yhat)\n",
    "        r2 = r2_score(y_test, yhat)\n",
    "        # store the result\n",
    "        cv_outer_results_mae.append(mae)\n",
    "        cv_outer_results_r2.append(r2)\n",
    "        # report progress\n",
    "        print('>mae=%.3f, r2=%.3f, est. mae=%.3f, cfg=%s' % (mae, r2, result.best_score_, result.best_params_))        \n",
    "    print('MAE: %.3f (%.3f) // R2: %.3f (%.3f)' % (np.mean(cv_outer_results_mae), np.std(cv_outer_results_mae), np.mean(cv_outer_results_r2), np.std(cv_outer_results_r2)))\n",
    "    \n",
    "    # Save CV results to DataFrame\n",
    "    df_results = pd.DataFrame.from_dict({'mae_splits': cv_outer_results_mae, 'r2_splits': cv_outer_results_r2})\n",
    "    df_results.to_csv(f'results/{exp_name}.csv')\n",
    "    \n",
    "    # Plot histogram of MAE and R2 estimates\n",
    "    _, axes = plt.subplots(1,1,figsize=(7,7))\n",
    "    axes.plot(range(len(cv_outer_results_r2)), cv_outer_results_r2)\n",
    "    axes.set_title(\"$R^2$ on all outer CV folds\", fontsize=18)\n",
    "    axes.set_xlabel(\"#fold\", fontsize=16)\n",
    "    axes.set_ylabel(\"$R^2$\", fontsize=16)\n",
    "    axes.set_xticks(list(range(10)))\n",
    "    axes.tick_params(labelsize=14)\n",
    "    padding = 100 if mae_plot_padding else 0\n",
    "    axes.set_ylim([-16,16])\n",
    "    axes.axhline(1, linestyle=\"--\", c='black')\n",
    "    axes.axhline(-1, linestyle=\"--\", c='black')\n",
    "    plt.savefig(f'plots/{exp_name}/r2_values_splits.svg')\n",
    "    plt.close()\n",
    "    \n",
    "    _, axes = plt.subplots(1,1,figsize=(7,7))\n",
    "    axes.plot(range(len(cv_outer_results_mae)), cv_outer_results_mae)\n",
    "    axes.set_title(\"$MAE$ on all outer CV folds\", fontsize=18)\n",
    "    axes.set_xlabel(\"#fold\", fontsize=16)\n",
    "    axes.set_ylabel(\"$MAE$\", fontsize=16)\n",
    "    axes.set_xticks(list(range(10)))\n",
    "    axes.tick_params(labelsize=14)\n",
    "    padding = 100 if mae_plot_padding else 0\n",
    "    axes.set_ylim([0,np.max(cv_outer_results_mae) + padding])\n",
    "    plt.savefig(f'plots/{exp_name}/mae_values_splits.svg')\n",
    "    plt.close()\n",
    "    \n",
    "    '''\n",
    "    # Plot actual vs. predicted values\n",
    "    _, ax = plt.subplots(figsize=(7,7))\n",
    "    display = PredictionErrorDisplay.from_predictions(y_true=target, y_pred=gs.predict(X), ax=ax, kind=\"actual_vs_predicted\")\n",
    "    ax.title.set_text(f'$R^2$ estimate: {gs.cv_results_[\"mean_test_r2\"][best_mae_idx]:.2f} ± {gs.cv_results_[\"std_test_r2\"][best_mae_idx]:.2f}\\nMAE estimate: {gs.cv_results_[\"mean_test_neg_mean_absolute_error\"][best_mae_idx]:.2f} ± {gs.cv_results_[\"std_test_neg_mean_absolute_error\"][best_mae_idx]:.2f}')\n",
    "    plt.savefig(f'plots/{exp_name}/prediction_error.svg')\n",
    "    '''\n",
    "    final_gs = get_gs()\n",
    "    final_result = final_gs.fit(X, target)\n",
    "    final_estimator = final_result.best_estimator_\n",
    "    print(f\"Model trained on all the dataset: {final_result.best_params_}\")\n",
    "    feature_names = final_estimator[:-1].get_feature_names_out()\n",
    "    cv_coef_var = cross_validate(\n",
    "        final_estimator,\n",
    "        X,\n",
    "        target,\n",
    "        cv=cv_inner,\n",
    "        return_estimator=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    final_estimator_coefs = pd.DataFrame(\n",
    "        [est[-1].coef_ for est in cv_coef_var[\"estimator\"]], columns=feature_names\n",
    "    )\n",
    "    \n",
    "    # Plot largest coefficients\n",
    "    coefs = pd.DataFrame(\n",
    "        final_estimator[-1].coef_,\n",
    "        columns=[\"Coefficients\"],\n",
    "        index=feature_names,\n",
    "    )\n",
    "\n",
    "    coefs['abs_coef'] = coefs['Coefficients'].abs()\n",
    "    coefs = coefs.sort_values(by='abs_coef', axis=0, ascending=False).head(20).drop('abs_coef', axis=1).iloc[::-1]\n",
    "\n",
    "    coefs.plot.barh(figsize=(9, 7))\n",
    "    plt.title(\"Best model\")\n",
    "    plt.axvline(x=0, color=\".5\")\n",
    "    plt.xlabel(\"Raw coefficient values\")\n",
    "    plt.subplots_adjust(left=0.3)\n",
    "    plt.savefig(f'plots/{exp_name}/coeff_values.svg')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot coefficient variance    \n",
    "    plt.figure(figsize=(9, len(coefs) * 0.8))\n",
    "    sns.stripplot(data=final_estimator_coefs[coefs.index.to_list()], orient=\"h\", palette=\"dark:k\", alpha=0.5, order=coefs.index.to_list()[::-1])\n",
    "    sns.boxplot(data=final_estimator_coefs[coefs.index.to_list()], orient=\"h\", color=\"cyan\", saturation=0.5, whis=10, order=coefs.index.to_list()[::-1])\n",
    "    plt.axvline(x=0, color=\".5\")\n",
    "    plt.title(\"Coefficient variability (most important coefficients)\")\n",
    "    plt.subplots_adjust(left=0.3)\n",
    "    plt.savefig(f'plots/{exp_name}/coeff_variability.svg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f7fa8-beed-4dfb-abbc-df45e019f195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp1 = Pipeline([('scaler', StandardScaler()),('model', LinearRegression())])\n",
    "\n",
    "params_exp1 = [\n",
    "    {'model': [LinearRegression()]},\n",
    "    {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "]\n",
    "\n",
    "run_exp(\"surface_scale_linear\", y_surf, pipeline_exp1, params_exp1, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f821f2-d240-4403-ad7d-f056bff6b929",
   "metadata": {},
   "source": [
    "## Experiment #2: Scaling + dimensionality reduction + linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0190c-6d1b-469c-93bc-70db9fa1dc38",
   "metadata": {},
   "source": [
    "As the preliminary analysis has shown, this dataset has 89 predictor variables, but only 50 samples. This high dimensionality may lead to overfitting issues, and therefore it could be interesting to try different dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab017e-4022-423f-81b1-e7a7a468dd81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp2 = Pipeline([('scaler', StandardScaler()), ('reduce_dims', PCA()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp2 = [\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [LinearRegression()]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "]\n",
    "\n",
    "run_exp(\"surface_scale_reduce_linear\", y_surf, pipeline_exp2, params_exp2, randomize=True, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44aff41-a0ea-41c3-a8e9-38123c5e3efc",
   "metadata": {},
   "source": [
    "## Experiment #3: Scaling + feature selection + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037ffa0-c1e2-4096-a0aa-1787f36a56a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp3 = Pipeline([('scaler', StandardScaler()), ('select_features', SelectKBest()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp3 = {\n",
    "    'feats': [\n",
    "        {'select_features': [SelectKBest(score_func=f_regression)], 'select_features__k': list(range(1,81))},\n",
    "        {'select_features': [RFE(LinearRegression())], 'select_features__n_features_to_select': list(range(1,81))}\n",
    "    ],\n",
    "    'regressor': [\n",
    "        {'model': [LinearRegression()]},\n",
    "        {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_exp3 = [flatten(params, parent_key=None) for params in dict_product(params_exp3)]\n",
    "\n",
    "run_exp(\"surface_scale_select_linear\", y_surf, pipeline_exp3, params_exp3, randomize=True, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650af52a-4dd9-48d9-9837-990455bf1e46",
   "metadata": {},
   "source": [
    "## Experiment #4: Scaling + feature selection + dimensionality reduction + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34b8f6-e48c-4ae5-bd1d-4e1ec545baae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp4 = Pipeline([('scaler', StandardScaler()), ('select_features', SelectKBest()), ('reduce_dims', PCA()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp4 = {\n",
    "    'feats': [\n",
    "        {'select_features': [SelectKBest(score_func=f_regression)], 'select_features__k': list(range(1,81))},\n",
    "        {'select_features': [RFE(LinearRegression())], 'select_features__n_features_to_select': list(range(1,81))}\n",
    "    ],\n",
    "    'dims': [\n",
    "        {'reduce_dims__n_components': list(range(1,31))}\n",
    "    ],\n",
    "    'regressor': [\n",
    "        {'model': [LinearRegression()]},\n",
    "        {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_exp4 = [flatten(params, parent_key=None) for params in dict_product(params_exp4)]\n",
    "run_exp(\"surface_scale_select_reduce_linear\", y_surf, pipeline_exp4, params_exp4, randomize=True, rs_iters=1000, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f57a9a-ab15-450c-8a8c-e4cdd20c1de9",
   "metadata": {},
   "source": [
    "# Experiments - pole middle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4fc6b-42a0-45f6-8645-37d5a2005e4f",
   "metadata": {},
   "source": [
    "## Experiment #1: Scaling + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdde42-a522-442c-a5f1-a3ac381cae37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp1 = Pipeline([('scaler', StandardScaler()),('model', LinearRegression())])\n",
    "\n",
    "params_exp1 = [\n",
    "    {'model': [LinearRegression()]},\n",
    "    {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "]\n",
    "\n",
    "run_exp(\"middle_scale_linear\", y_mid, pipeline_exp1, params_exp1, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b180f-70b5-4c05-98a2-1d31573f242b",
   "metadata": {},
   "source": [
    "## Experiment #2: Scaling + dimensionality reduction + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bcfbcf-ccc7-4bf3-a03d-a16870a82827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp2 = Pipeline([('scaler', StandardScaler()), ('reduce_dims', PCA()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp2 = [\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [LinearRegression()]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "]\n",
    "\n",
    "run_exp(\"middle_scale_reduce_linear\", y_mid, pipeline_exp2, params_exp2, randomize=True, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8026d52-cbba-4d8b-ab11-00273ef36cc8",
   "metadata": {},
   "source": [
    "## Experiment #3: Scaling + feature selection + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68b6e3-bded-41fb-9b85-033b9a3c6453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp3 = Pipeline([('scaler', StandardScaler()), ('select_features', SelectKBest()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp3 = {\n",
    "    'feats': [\n",
    "        {'select_features': [SelectKBest(score_func=f_regression)], 'select_features__k': list(range(1,81))},\n",
    "        {'select_features': [RFE(LinearRegression())], 'select_features__n_features_to_select': list(range(1,81))}\n",
    "    ],\n",
    "    'regressor': [\n",
    "        {'model': [LinearRegression()]},\n",
    "        {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_exp3 = [flatten(params, parent_key=None) for params in dict_product(params_exp3)]\n",
    "\n",
    "run_exp(\"middle_scale_select_linear\", y_mid, pipeline_exp3, params_exp3, randomize=True, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08e14c-dfc3-400a-b427-a67978d74d23",
   "metadata": {},
   "source": [
    "## Experiment #4: Scaling + feature selection + dimensionality reduction + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feae0fa-196d-4b16-9a2e-e857cef6a048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp4 = Pipeline([('scaler', StandardScaler()), ('select_features', SelectKBest()), ('reduce_dims', PCA()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp4 = {\n",
    "    'feats': [\n",
    "        {'select_features': [SelectKBest(score_func=f_regression)], 'select_features__k': list(range(1,81))},\n",
    "        {'select_features': [RFE(LinearRegression())], 'select_features__n_features_to_select': list(range(1,81))}\n",
    "    ],\n",
    "    'dims': [\n",
    "        {'reduce_dims__n_components': list(range(1,31))}\n",
    "    ],\n",
    "    'regressor': [\n",
    "        {'model': [LinearRegression()]},\n",
    "        {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_exp4 = [flatten(params, parent_key=None) for params in dict_product(params_exp4)]\n",
    "run_exp(\"middle_scale_select_reduce_linear\", y_mid, pipeline_exp4, params_exp4, randomize=True, rs_iters=1000, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890be415-6f64-4b86-b577-8ae0a7a080d4",
   "metadata": {},
   "source": [
    "# Experiments - pole interior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20aaf62-2d8d-4887-b7b0-838af5fcbaa7",
   "metadata": {},
   "source": [
    "## Experiment #1: Scaling + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11cc00-dd93-42fe-8d8e-ab54eab2e5bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp1 = Pipeline([('scaler', StandardScaler()),('model', LinearRegression())])\n",
    "\n",
    "params_exp1 = [\n",
    "    {'model': [LinearRegression()]},\n",
    "    {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "]\n",
    "\n",
    "run_exp(\"interior_scale_linear\", y_int, pipeline_exp1, params_exp1, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2118c-3aef-44fa-a27a-712b3ed4bc6a",
   "metadata": {},
   "source": [
    "## Experiment #2: Scaling + dimensionality reduction + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b70878-79fb-47d7-9e91-d4d8953a3514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp2 = Pipeline([('scaler', StandardScaler()), ('reduce_dims', PCA()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp2 = [\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [LinearRegression()]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "]\n",
    "\n",
    "run_exp(\"interior_scale_reduce_linear\", y_int, pipeline_exp2, params_exp2, randomize=True, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b0b7d-bad0-4abb-8caa-dbe1693ab4ae",
   "metadata": {},
   "source": [
    "In this case, the obtained model seems to perform slightly better than the previous one. The estimated $R^2$ score has improved from 0.29 to 0.35, and the MAE has improved from 1114 to 1092. It is a small improvement, but a sign that dimensionality reduction might be good for this problem, even if model interpretability becomes harder this way.\n",
    "\n",
    "However, by looking at the coefficient variability plot, it can be seen that coefficients vary much more wildly than in the previous setting, so the coefficients obtained by the model trained upon the dimensionality-reduced dataset might not be as reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2ebbf-9f87-470d-96ce-6f9010bcc307",
   "metadata": {},
   "source": [
    "## Experiment #3: Scaling + feature selection + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee4e0b-34e0-4aac-afc1-3a1237c567a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp3 = Pipeline([('scaler', StandardScaler()), ('select_features', SelectKBest()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp3 = {\n",
    "    'feats': [\n",
    "        {'select_features': [SelectKBest(score_func=f_regression)], 'select_features__k': list(range(1,81))},\n",
    "        {'select_features': [RFE(LinearRegression())], 'select_features__n_features_to_select': list(range(1,81))}\n",
    "    ],\n",
    "    'regressor': [\n",
    "        {'model': [LinearRegression()]},\n",
    "        {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_exp3 = [flatten(params, parent_key=None) for params in dict_product(params_exp3)]\n",
    "\n",
    "run_exp(\"interior_scale_select_linear\", y_int, pipeline_exp3, params_exp3, randomize=True, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbff9d-306f-4210-a552-240daf0350b5",
   "metadata": {},
   "source": [
    "## Experiment #4: Scaling + feature selection + dimensionality reduction + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06613015-c430-4018-8e22-74da5156d866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp4 = Pipeline([('scaler', StandardScaler()), ('select_features', SelectKBest()), ('reduce_dims', PCA()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp4 = {\n",
    "    'feats': [\n",
    "        {'select_features': [SelectKBest(score_func=f_regression)], 'select_features__k': list(range(1,81))},\n",
    "        {'select_features': [RFE(LinearRegression())], 'select_features__n_features_to_select': list(range(1,81))}\n",
    "    ],\n",
    "    'dims': [\n",
    "        {'reduce_dims__n_components': list(range(1,31))}\n",
    "    ],\n",
    "    'regressor': [\n",
    "        {'model': [LinearRegression()]},\n",
    "        {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_exp4 = [flatten(params, parent_key=None) for params in dict_product(params_exp4)]\n",
    "run_exp(\"interior_scale_select_reduce_linear\", y_int, pipeline_exp4, params_exp4, randomize=True, rs_iters=1000, random_state=RS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bce70c-3382-4a8f-8cc9-d35139af74c2",
   "metadata": {},
   "source": [
    "# Experiments - surface depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124608d5-51e1-4f0d-8034-0dbc19c04352",
   "metadata": {},
   "source": [
    "## Experiment #1: Scaling + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2df1a-01b3-4329-b196-b8d1744a3442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp1 = Pipeline([('scaler', StandardScaler()),('model', LinearRegression())])\n",
    "\n",
    "params_exp1 = [\n",
    "    {'model': [LinearRegression()]},\n",
    "    {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "]\n",
    "\n",
    "run_exp(\"depth_scale_linear\", y_sdepth, pipeline_exp1, params_exp1, random_state=RS, mae_plot_padding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455b64e-d85e-4826-b665-c4d46b543f3a",
   "metadata": {},
   "source": [
    "## Experiment #2: Scaling + dimensionality reduction + linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587fccb-5f63-4bf1-9980-51629f8aa6ba",
   "metadata": {},
   "source": [
    "As the preliminary analysis has shown, this dataset has 89 predictor variables, but only 50 samples. This high dimensionality may lead to overfitting issues, and therefore it could be interesting to try different dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa420fc-56ee-41e5-9276-d8fc05e95c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp2 = Pipeline([('scaler', StandardScaler()), ('reduce_dims', PCA()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp2 = [\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [LinearRegression()]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "    {'reduce_dims__n_components': list(range(1,31)), 'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "]\n",
    "\n",
    "run_exp(\"depth_scale_reduce_linear\", y_sdepth, pipeline_exp2, params_exp2, randomize=True, random_state=RS, mae_plot_padding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9988470-6730-4f93-9295-9185ce63ed20",
   "metadata": {},
   "source": [
    "## Experiment #3: Scaling + feature selection + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb05da-9b8b-4aa2-abf7-5d5a30e4eeba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp3 = Pipeline([('scaler', StandardScaler()), ('select_features', SelectKBest()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp3 = {\n",
    "    'feats': [\n",
    "        {'select_features': [SelectKBest(score_func=f_regression)], 'select_features__k': list(range(1,81))},\n",
    "        {'select_features': [RFE(LinearRegression())], 'select_features__n_features_to_select': list(range(1,81))}\n",
    "    ],\n",
    "    'regressor': [\n",
    "        {'model': [LinearRegression()]},\n",
    "        {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_exp3 = [flatten(params, parent_key=None) for params in dict_product(params_exp3)]\n",
    "\n",
    "run_exp(\"depth_scale_select_linear\", y_sdepth, pipeline_exp3, params_exp3, randomize=True, random_state=RS, mae_plot_padding=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bb3b6-8fc7-450d-9feb-ce8137ca8b05",
   "metadata": {},
   "source": [
    "## Experiment #4: Scaling + feature selection + dimensionality reduction + linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a4bb4-af04-4e3d-ae15-56bd973833fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_exp4 = Pipeline([('scaler', StandardScaler()), ('select_features', SelectKBest()), ('reduce_dims', PCA()), ('model', LinearRegression())])\n",
    "\n",
    "params_exp4 = {\n",
    "    'feats': [\n",
    "        {'select_features': [SelectKBest(score_func=f_regression)], 'select_features__k': list(range(1,81))},\n",
    "        {'select_features': [RFE(LinearRegression())], 'select_features__n_features_to_select': list(range(1,81))}\n",
    "    ],\n",
    "    'dims': [\n",
    "        {'reduce_dims__n_components': list(range(1,31))}\n",
    "    ],\n",
    "    'regressor': [\n",
    "        {'model': [LinearRegression()]},\n",
    "        {'model': [Ridge()], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [Lasso(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]},\n",
    "        {'model': [ElasticNet(max_iter=100000, random_state=RS)], 'model__alpha': [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "params_exp4 = [flatten(params, parent_key=None) for params in dict_product(params_exp4)]\n",
    "run_exp(\"depth_scale_select_reduce_linear\", y_sdepth, pipeline_exp4, params_exp4, randomize=True, rs_iters=1000, random_state=RS, mae_plot_padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef072a2d-5174-478c-8e7b-574e6f23e8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
